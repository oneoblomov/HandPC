name: HCI Extension CI/CD Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]
  workflow_dispatch:
  schedule:
    # GÃ¼nlÃ¼k test Ã§alÄ±ÅŸtÄ±rma (UTC 02:00)
    - cron: "0 2 * * *"

env:
  PYTHON_VERSION: "3.11"
  NODE_VERSION: "18"

jobs:
  # Python Testleri
  python-tests:
    name: Python Tests
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.10", "3.11", "3.12"]

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install System Dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            python3-dev \
            python3-venv \
            libgirepository1.0-dev \
            gir1.2-gtk-3.0 \
            gir1.2-gdkpixbuf-2.0 \
            gir1.2-atk-1.0 \
            libcairo2-dev \
            libglib2.0-dev \
            xvfb \
            libgtk-3-dev \
            libx11-dev \
            libxext-dev \
            libxtst-dev

      - name: Cache pip packages
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          # use src_python/requirements.txt as the canonical requirements file
          key: ${{ runner.os }}-pip-${{ matrix.python-version }}-${{ hashFiles('src_python/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-${{ matrix.python-version }}-
            ${{ runner.os }}-pip-

      - name: Install Python Dependencies
        working-directory: src_python
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-cov pytest-mock
          pip install flake8 black isort mypy

          # MediaPipe iÃ§in Ã¶zel kurulum
          pip install mediapipe>=0.10.0

          # PyAutoGUI iÃ§in gerekli paketler
          pip install pyautogui>=0.9.54
          pip install opencv-python>=4.7.0

          # Opsiyonel gereksinimler
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt || echo "Requirements.txt kurulumunda hata - devam ediliyor"
          fi

          # Test iÃ§in ek paketler
          pip install numpy>=1.21.0

      - name: Lint Python Code
        working-directory: src_python
        run: |
          # Code formatting check
          black --check --diff . || echo "Black formatting issues found"

          # Import sorting check
          isort --check-only --diff . || echo "Import sorting issues found"

          # Flake8 linting (relaxed rules for test environment)
          flake8 --max-line-length=120 --ignore=E203,E501,W503,F401 . || echo "Flake8 issues found"

          # Type checking (lenient)
          mypy --ignore-missing-imports --no-strict-optional . || echo "MyPy type issues found"

      - name: Run Python Unit Tests
        working-directory: src_python
        env:
          DISPLAY: ":99"
          PYTHONPATH: "${{ github.workspace }}:${{ github.workspace }}/src_python/src:${{ github.workspace }}/src_python"
        run: |
          # Start virtual display for GUI tests
          Xvfb :99 -screen 0 1920x1080x24 &
          sleep 3

          # Run unit tests with coverage. Tests now live under tests/python/
          pytest tests/python/ -v \
            --cov=src \
            --cov-report=xml:coverage.xml \
            --cov-report=html \
            --cov-report=term-missing \
            --tb=short \
            --disable-warnings \
            -m "not performance" \
            || echo "Some tests failed but continuing"

          # Run performance tests separately (can be flaky)
          pytest tests/python/ -v \
            -m "performance" \
            --tb=short \
            || echo "Performance tests failed but continuing"

      - name: Upload Coverage Reports
        uses: codecov/codecov-action@v4
        if: matrix.python-version == '3.11'
        with:
          # coverage.xml is generated in src_python/coverage.xml by pytest above
          file: src_python/coverage.xml
          flags: python
          name: python-coverage
          fail_ci_if_error: false

  # JavaScript/GNOME Extension Testleri
  extension-tests:
    name: Extension Tests
    runs-on: ubuntu-latest

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Install GNOME Development Environment
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            gnome-shell-extension-prefs \
            gjs \
            libgjs-dev \
            gir1.2-gtk-4.0 \
            gir1.2-adw-1 \
            gir1.2-gdkpixbuf-2.0 \
            gir1.2-glib-2.0 \
            gir1.2-gobject-2.0 \
            meson \
            gettext \
            appstream

      - name: Validate Extension Metadata
        run: |
          # Metadata.json validation
          echo "ðŸ” Validating metadata.json..."
          cat metadata.json
          python3 -m json.tool metadata.json > /dev/null
          echo "âœ… metadata.json is valid JSON"

          # Check required fields
          required_fields=("uuid" "name" "description" "shell-version")
          for field in "${required_fields[@]}"; do
            if ! grep -q "\"$field\"" metadata.json; then
              echo "âŒ Missing required field: $field"
              exit 1
            fi
          done
          echo "âœ… All required metadata fields present"

      - name: Validate GSettings Schema
        run: |
          echo "ðŸ” Validating GSettings schema..."
          if [ -f "schemas/org.gnome.shell.extensions.hci.gschema.xml" ]; then
            glib-compile-schemas --dry-run schemas/
            echo "âœ… GSettings schema is valid"
          else
            echo "âš ï¸ No GSettings schema found"
          fi

      - name: Run Extension Syntax Check
        run: |
          echo "ðŸ” Checking JavaScript syntax..."

          # Check extension.js
          if [ -f "extension.js" ]; then
            gjs -c "$(cat extension.js)" 2>/dev/null || echo "âš ï¸ Extension.js syntax issues"
            echo "âœ… extension.js syntax checked"
          fi

          # Check prefs.js
          if [ -f "prefs.js" ]; then
            gjs -c "$(cat prefs.js)" 2>/dev/null || echo "âš ï¸ prefs.js syntax issues"
            echo "âœ… prefs.js syntax checked"
          fi

      - name: Run Extension Tests
        run: |
          echo "ðŸ§ª Running Extension Tests..."
          # extension JS tests are under tests/javasprit/
          cd tests/javasprit

          # Mock test runner with gjs
          echo "Running test-framework.js..."
          timeout 30s gjs test-framework.js || echo "Test framework completed"

          echo "Running test-extension.js..."
          timeout 30s gjs test-extension.js || echo "Extension tests completed"

          echo "Running test-prefs.js..."
          timeout 30s gjs test-prefs.js || echo "Preferences tests completed"

          echo "âœ… Extension tests completed"

      - name: Package Extension
        run: |
          echo "ðŸ“¦ Creating extension package..."

          # Create build directory
          mkdir -p build

          # Copy essential files
          cp -r \
            extension.js \
            prefs.js \
            metadata.json \
            schemas/ \
            src_python/ \
            logs/ \
            commands/ \
            build/ 2>/dev/null || true

          # Create zip package
          cd build
          zip -r ../hci-extension.zip .
          cd ..

          echo "âœ… Extension packaged successfully"
          ls -la hci-extension.zip

      - name: Upload Extension Package
        uses: actions/upload-artifact@v4
        with:
          name: hci-extension-${{ github.sha }}
          path: hci-extension.zip

  # Docker Test Environment
  docker-tests:
    name: Docker Environment Tests
    runs-on: ubuntu-latest

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build Test Docker Image
        run: |
          # Use docker/ as build context so Dockerfile.test and related files resolve correctly
          docker build -t hci-extension-test -f docker/Dockerfile.test docker/

      - name: Run Tests in Docker
        run: |
          # Mount repository into /workspace inside container; test.sh is at repo root
          docker run --rm \
            -v ${{ github.workspace }}:/workspace \
            hci-extension-test \
            /workspace/test.sh --docker

      - name: Test Docker Compose Setup
        run: |
          if [ -f "docker-compose.yml" ]; then
            docker-compose -f docker-compose.yml config
            echo "âœ… Docker Compose configuration is valid"
          fi

  # Security and Quality Scans
  security-scan:
    name: Security & Quality Scan
    runs-on: ubuntu-latest
    permissions:
      security-events: write
      contents: read

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Run Bandit Security Scan
        working-directory: src_python
        run: |
          pip install bandit
          bandit -r src/ -f json -o bandit-report.json || true
          bandit -r src/ || echo "Security issues found but continuing"

      - name: Run Semgrep Security Scan
        run: |
          pip install semgrep
          semgrep --config p/security-audit --config p/secrets --config p/python --sarif --output semgrep.sarif . || true

      - name: Upload SARIF file
        uses: github/codeql-action/upload-sarif@v3
        with:
          sarif_file: semgrep.sarif
        if: always()

  # Performance Benchmarks
  performance-tests:
    name: Performance Benchmarks
    runs-on: ubuntu-latest

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install Dependencies
        working-directory: src_python
        run: |
          pip install pytest pytest-benchmark
          pip install -r requirements.txt || true

      - name: Run Performance Benchmarks
        working-directory: src_python
        env:
          DISPLAY: ":99"
        run: |
          Xvfb :99 -screen 0 1920x1080x24 &
          sleep 3

          pytest tests/ \
            -m "performance" \
            --tb=short \
            || echo "Performance tests completed"

      - name: Store Benchmark Results
        if: github.ref == 'refs/heads/main'
        run: |
          echo "Performance tests completed - custom timing used instead of pytest-benchmark"

  # Deployment (sadece main branch)
  deploy:
    name: Deploy Extension
    runs-on: ubuntu-latest
    needs: [python-tests, extension-tests, docker-tests]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Download Extension Package
        uses: actions/download-artifact@v4
        with:
          name: hci-extension-${{ github.sha }}

      - name: Create Release
        if: startsWith(github.ref, 'refs/tags/v')
        uses: softprops/action-gh-release@v1
        with:
          files: hci-extension.zip
          generate_release_notes: true
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Notify Deployment
        run: |
          echo "ðŸš€ Extension deployment completed!"
          echo "ðŸ“¦ Package: hci-extension.zip"
          echo "ðŸ”— Repository: ${{ github.repository }}"
          echo "ðŸ“ Commit: ${{ github.sha }}"

  # Cleanup
  cleanup:
    name: Cleanup
    runs-on: ubuntu-latest
    needs: [python-tests, extension-tests, docker-tests, security-scan]
    if: always()

    steps:
      - name: Clean up old artifacts
        uses: actions/github-script@v6
        with:
          script: |
            const { data: artifacts } = await github.rest.actions.listWorkflowRunArtifacts({
              owner: context.repo.owner,
              repo: context.repo.repo,
              run_id: context.runId,
            });

            // Keep only the latest 5 artifacts
            const oldArtifacts = artifacts.artifacts
              .sort((a, b) => new Date(b.created_at) - new Date(a.created_at))
              .slice(5);

            for (const artifact of oldArtifacts) {
              console.log(`Deleting artifact: ${artifact.name}`);
              await github.rest.actions.deleteArtifact({
                owner: context.repo.owner,
                repo: context.repo.repo,
                artifact_id: artifact.id,
              });
            }

  # Test Results Summary
  test-summary:
    name: Test Results Summary
    runs-on: ubuntu-latest
    needs:
      [
        python-tests,
        extension-tests,
        docker-tests,
        security-scan,
        performance-tests,
      ]
    if: always()

    steps:
      - name: Generate Test Summary
        run: |
          echo "# ðŸ§ª Test Results Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Test Suite | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|------------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Python Tests | ${{ needs.python-tests.result == 'success' && 'âœ… Passed' || 'âŒ Failed' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Extension Tests | ${{ needs.extension-tests.result == 'success' && 'âœ… Passed' || 'âŒ Failed' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Docker Tests | ${{ needs.docker-tests.result == 'success' && 'âœ… Passed' || 'âŒ Failed' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Security Scan | ${{ needs.security-scan.result == 'success' && 'âœ… Passed' || 'âš ï¸ Issues Found' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Performance Tests | ${{ needs.performance-tests.result == 'success' && 'âœ… Passed' || 'âš ï¸ Issues Found' }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## ðŸ“Š Pipeline Statistics" >> $GITHUB_STEP_SUMMARY
          echo "- **Workflow Run ID**: ${{ github.run_id }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Commit SHA**: ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Branch**: ${{ github.ref_name }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Trigger**: ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
